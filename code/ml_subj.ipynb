{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1tL86eZdW5OnTHFOtEIaZev88mneSqwyO","timestamp":1767532691347},{"file_id":"1b5dL3bM9WcGc_j6DvDMWDhjMtqNeCUYw","timestamp":1767517350271}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"odpCeHVK9sud","executionInfo":{"status":"ok","timestamp":1769053625513,"user_tz":-420,"elapsed":30325,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}},"outputId":"9abc6bf6-7348-4b21-aceb-003ee85d3480"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":true,"id":"6UNCz6Iz9m0L","executionInfo":{"status":"ok","timestamp":1769053636250,"user_tz":-420,"elapsed":6837,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b1f78bbb-de85-488b-9df2-7265f70e69a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["! pip install faiss-cpu sentence_transformers tokenizers transformers -q"]},{"cell_type":"code","source":["import os\n","os.environ[\"TRANSFORMERS_NO_MISTRAL_REGEX_PATCH\"] = \"1\""],"metadata":{"id":"k3xcLNttK8uX","executionInfo":{"status":"ok","timestamp":1769053726957,"user_tz":-420,"elapsed":8,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import json\n","import copy\n","import faiss\n","import numpy as np\n","from sentence_transformers import SentenceTransformer, CrossEncoder\n","from transformers import AutoTokenizer"],"metadata":{"id":"6YnGuSF-EcOC","executionInfo":{"status":"ok","timestamp":1769053755977,"user_tz":-420,"elapsed":26746,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["OUT_DIR = \"/content/drive/MyDrive/rag\"\n","EMBED_MODEL_PATH = f\"{OUT_DIR}/bge-m3-rag-finetuned\"\n","RERANKER_MODEL = f\"{OUT_DIR}/bge-m3-reranker-finetuned\""],"metadata":{"id":"9WQI3sSu-CJg","executionInfo":{"status":"ok","timestamp":1769054783993,"user_tz":-420,"elapsed":145,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def retrieve(embedder, query, top_k=20, window=1):\n","    q_emb = embedder.encode(\n","        query,\n","        normalize_embeddings=True\n","    ).astype(\"float32\")\n","\n","    scores, indices = index.search(\n","        np.expand_dims(q_emb, axis=0),\n","        top_k\n","    )\n","\n","    results = []\n","    n_docs = len(docs)\n","\n","    for score, idx in zip(scores[0], indices[0]):\n","        start = max(0, idx - window)\n","        end = min(n_docs, idx + window + 1)\n","\n","        surrounding_docs = [\n","            {\n","                \"doc_idx\": i,\n","                \"doc\": docs[i]\n","            }\n","            for i in range(start, end)\n","            if i != idx\n","        ]\n","\n","        results.append({\n","            \"score\": float(score),\n","            \"doc_idx\": idx,\n","            \"doc\": docs[idx],\n","            \"surrounding\": surrounding_docs\n","        })\n","\n","    return results\n"],"metadata":{"id":"Lv1sKF8_Kov3","executionInfo":{"status":"ok","timestamp":1769054788779,"user_tz":-420,"elapsed":42,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def truncate_for_reranker(tokenizer, query, doc, max_length=512):\n","    encoded = tokenizer(\n","        query,\n","        doc,\n","        truncation=\"only_second\",\n","        max_length=max_length,\n","        return_tensors=None\n","    )\n","    return tokenizer.decode(\n","        encoded[\"input_ids\"],\n","        skip_special_tokens=True\n","    )"],"metadata":{"id":"M0y7OosCKnt4","executionInfo":{"status":"ok","timestamp":1769054850252,"user_tz":-420,"elapsed":44,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def rerank(reranker, tokenizer, query, retrieved, top_k=5):\n","    pairs = []\n","    for r in retrieved:\n","        text = truncate_for_reranker(tokenizer, query, r[\"doc\"][\"text\"])\n","        pairs.append((query, text))\n","\n","    scores = reranker.predict(pairs)\n","\n","    for r, s in zip(retrieved, scores):\n","        r[\"rerank_score\"] = float(s)\n","\n","    reranked = sorted(\n","        retrieved,\n","        key=lambda x: x[\"rerank_score\"],\n","        reverse=True\n","    )\n","\n","    return reranked[:top_k]"],"metadata":{"id":"ZaDfT4WZKp-1","executionInfo":{"status":"ok","timestamp":1769054957544,"user_tz":-420,"elapsed":43,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def pipeline(query, embedder, reranker, reranker_tokenizer, retrieve_k=40, rerank_k=20, expand_surrondings=True):\n","    retrieved = retrieve(embedder, query, top_k=retrieve_k)\n","    reranked = rerank(reranker, reranker_tokenizer, query, retrieved, top_k=rerank_k)\n","\n","    if expand_surrondings:\n","        for r in reranked:\n","            r[\"doc\"] = copy.deepcopy(r[\"doc\"])\n","\n","            seen_idx = set()\n","            ordered_chunks = []\n","\n","            idx = r[\"doc_idx\"]\n","            if idx not in seen_idx:\n","                seen_idx.add(idx)\n","                ordered_chunks.append({\n","                    \"doc_idx\": idx,\n","                    \"text\": r[\"doc\"][\"text\"]\n","                })\n","\n","            for s in r.get(\"surrounding\", []):\n","                s_idx = s[\"doc_idx\"]\n","                if s_idx not in seen_idx:\n","                    seen_idx.add(s_idx)\n","                    ordered_chunks.append({\n","                        \"doc_idx\": s_idx,\n","                        \"text\": s[\"doc\"][\"text\"]\n","                    })\n","\n","            ordered_chunks.sort(key=lambda x: x[\"doc_idx\"])\n","\n","            r[\"doc\"][\"text\"] = \"\\n\".join(c[\"text\"] for c in ordered_chunks)\n","\n","    return reranked\n"],"metadata":{"id":"zl7oiaUHKsT5","executionInfo":{"status":"ok","timestamp":1769054964050,"user_tz":-420,"elapsed":16,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["embedder = SentenceTransformer(EMBED_MODEL_PATH)\n","index = faiss.read_index(f\"{OUT_DIR}/rag_finetuned.index\")\n","\n","with open(f\"{OUT_DIR}/metadata_finetuned.json\", \"r\", encoding=\"utf-8\") as f:\n","    docs = json.load(f)\n","\n","assert index.ntotal == len(docs)\n","\n","reranker = CrossEncoder(RERANKER_MODEL)\n","reranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"dKdNRZLd-FJC","executionInfo":{"status":"error","timestamp":1769056072078,"user_tz":-420,"elapsed":55,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}},"outputId":"aa71d8ee-f977-44e5-ab2a-359cb2b1944a"},"execution_count":14,"outputs":[{"output_type":"error","ename":"OSError","evalue":"Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /content/drive/MyDrive/rag/bge-m3-rag-finetuned.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1816354648.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBED_MODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{OUT_DIR}/rag_finetuned.index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{OUT_DIR}/metadata_finetuned.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             ):\n\u001b[0;32m--> 327\u001b[0;31m                 modules, self.module_kwargs = self._load_sbert_model(\n\u001b[0m\u001b[1;32m    328\u001b[0m                     \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m_load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   2303\u001b[0m                 \u001b[0;31m# Newer modules that support the new loading method are loaded with the new style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2304\u001b[0m                 \u001b[0;31m# i.e. with many keyword arguments that can optionally be used by the modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2305\u001b[0;31m                 module = module_class.load(\n\u001b[0m\u001b[1;32m   2306\u001b[0m                     \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2307\u001b[0m                     \u001b[0;31m# Loading-specific keyword arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, model_name_or_path, subfolder, token, cache_folder, revision, local_files_only, trust_remote_code, model_kwargs, tokenizer_kwargs, config_kwargs, backend, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         )\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_peft_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_peft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Get the signature of the auto_model's forward method to pass only the expected arguments from `features`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36m_load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_mt5_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 self.auto_model = AutoModel.from_pretrained(\n\u001b[0m\u001b[1;32m    198\u001b[0m                     \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4898\u001b[0m             )\n\u001b[1;32m   4899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4900\u001b[0;31m         checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n\u001b[0m\u001b[1;32m   4901\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4902\u001b[0m             \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[0m\n\u001b[1;32m    987\u001b[0m                 )\n\u001b[1;32m    988\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 raise OSError(\n\u001b[0m\u001b[1;32m    990\u001b[0m                     \u001b[0;34mf\"Error no file named {_add_variant(WEIGHTS_NAME, variant)}, {_add_variant(SAFE_WEIGHTS_NAME, variant)},\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                     \u001b[0;34mf\" {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME + '.index'} or {FLAX_WEIGHTS_NAME} found in directory\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /content/drive/MyDrive/rag/bge-m3-rag-finetuned."]}]},{"cell_type":"code","source":["query = \"chứng chỉ tiếng anh gì thì được miễn học phần tiếng anh\"\n","\n","results = pipeline(query, embedder, reranker, reranker_tokenizer, retrieve_k=40, rerank_k=15)\n","\n","for i, r in enumerate(results, 1):\n","    print(f\"\\nRank {i}\")\n","    print(\"Rerank score:\", r[\"rerank_score\"])\n","    print(\"Source:\", r[\"doc\"][\"metadata\"][\"source\"])\n","    print(r[\"doc\"][\"text\"])"],"metadata":{"id":"MIH73_7MKx_p","executionInfo":{"status":"aborted","timestamp":1769053569085,"user_tz":-420,"elapsed":27308,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}},"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install pydantic groq docxtpl -q"],"metadata":{"collapsed":true,"id":"eFgC8Bh19TPf","executionInfo":{"status":"aborted","timestamp":1769053569088,"user_tz":-420,"elapsed":27307,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import re\n","from pathlib import Path\n","from google.colab import userdata\n","from typing import Dict, List, Optional, Any\n","from pydantic import BaseModel, Field\n","from docxtpl import DocxTemplate, RichText\n","from groq import Groq"],"metadata":{"id":"21mNLX4sF-wa","executionInfo":{"status":"aborted","timestamp":1769053569091,"user_tz":-420,"elapsed":27309,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata\n","\n","FORMS_PATH = \"/content/drive/MyDrive/rag/forms\"\n","GROQ_API_KEY = userdata.get('groqApiKey')\n","\n","CHAT_LLM = \"llama-3.3-70b-versatile\"\n","GUIDE_LLM = \"llama-3.3-70b-versatile\"\n","EXTRACTOR_LLM = \"llama-3.3-70b-versatile\"\n","INTENT_LLM = \"llama-3.3-70b-versatile\"\n","\n","CHECKER_LLM = \"llama-3.1-8b-instant\"\n","SUM_LLM = \"llama-3.1-8b-instant\"\n","\n","CONFIDENCE_THRESHOLD = 0.75\n","\n","MOCK_USER_DATA = {\n","    \"Thông tin sinh viên\": {\n","        \"họ tên\": \"Phạm Thành Khương\",\n","        \"mssv\": \"20241234\",\n","        \"khóa\": \"69\",\n","        \"lớp\": \"Khoa học Máy tính 01\",\n","        \"khoa\": \"Công nghệ Thông tin\",\n","        \"viện\": \"CNTT&TT\",\n","        \"email\": \"so1bk@hust.edu.vn\",\n","        \"ngày sinh\": \"12/05/2006 (dd/mm/yyyy)\",\n","        \"giới tính\": \"Nam\",\n","    },\n","    \"Tình trạng học tập\": {\n","        \"đang theo học tại trường\": True,\n","        \"cpa\": \"3.5\",\n","    }\n","}\n","\n","CHAT_PROMPT = \"\"\"\n","THÔNG TIN TRA CỨU TỪ TÀI LIỆU NHÀ TRƯỜNG (RAG):\n","\n","{rag_context}\n","\n","==== KẾT THÚC THÔNG TIN TRA CỨU TỪ TÀI LIỆU NHÀ TRƯỜNG ====\n","\n","Bạn là **hệ thống Chat Bot hỗ trợ sinh viên Đại học Bách Khoa Hà Nội (HUST)**.\n","Bạn đang trò chuyện với sinh viên ĐANG THEO HỌC tại Đại học Bách Khoa Hà Nội.\n","Chức năng chính của bạn là hỗ trợ, trả lời các câu hỏi và cung cấp thông tin cho sinh viên.\n","Ngoài ra, hệ thống có thể tạo các đơn liên quan tới Đại học Bách Khoa Hà Nội, biểu mẫu nếu được yêu cầu nằm trong DANH SÁCH BIỂU MẪU (nếu biểu mẫu sinh viên mong muốn không nằm trong đó thì suy luận và đưa phương án khác).\n","Từ nội dung tin nhắn và tóm tắt, suy luận xem sinh viên có cần tạo ra biểu mẫu nào trong DANH SÁCH BIÊU MẪU thì hãy đề xuất.\n","\n","DANH SÁCH BIỂU MẪU:\n","{form_list}\n","\n","DỮ LIỆU CỦA SINH VIÊN:\n","{user_data}\n","\n","TÓM TẮT TRÒ CHUYỆN:\n","{summary}\n","\n","TIN NHẮN TRƯỚC CỦA HỆ THỐNG TỚI SINH VIÊN:\n","\"{prev_sys_message}\"\n","\n","TIN NHẮN CỦA SINH VIÊN:\n","\"{user_message}\"\n","\n","NHIỆM VỤ:\n","- Trả lời LỊCH SỰ, TRỌNG TÂM như cán bộ hỗ trợ sinh viên.\n","- Trả lời bằng ngôn ngữ viết trang trọng và KHÔNG bao gồm các ký hiệu, tên biến, ngôn ngữ lập trình.\n","- Trình diễn câu trả lời dễ đọc, cách dòng, lề đầy đủ.\n","- Ưu tiên dùng RAG, thông tin tra cứu từ tài liệu nhà trường.\n","- Bỏ qua tài liệu RAG không liên quan.\n","- Trích dẫn nguồn nếu có thể.\n","- Sau khi trả lời, xem xét lại xem có biểu mẫu nào có thể sinh viên cần không thì hãy đề xuất.\n","\"\"\"\n","\n","INTENT_PROMPT = \"\"\"\n","Bạn là **hệ thống Chat Bot hỗ trợ sinh viên Đại học Bách Khoa Hà Nội (HUST)** trả lời bằng JSON được chỉ định.\n","Chức năng chính của bạn là hỗ trợ, trả lời các câu hỏi và cung cấp thông tin cho sinh viên.\n","Ngoài ra, hệ thống có thể tạo các đơn liên quan tới Đại học Bách Khoa Hà Nội, biểu mẫu nếu được yêu cầu nằm trong DANH SÁCH BIỂU MẪU.\n","\n","NHIỆM VỤ:\n","- Phân tích tin nhắn người dùng và tóm tắt trò chuyện để xem họ có cần làm biểu mẫu hay không, và nếu có thì biểu mẫu nào.\n","- Nếu không có biểu mẫu nào phù hợp với ý định của sinh viên thi để form_name là null.\n","- is_recommend là true nếu người dùng chưa rõ cần lập đơn nào, và bạn đưa ra đề xuất.\n","- is_recommend là false nếu người dùng đang yêu cầu lập đơn đã mô tả.\n","\n","DANH SÁCH BIỂU MẪU:\n","(Nếu không có biểu mẫu nào phù hợp trực tiếp thì để form_name là null)\n","{form_list}\n","\n","TÓM TẮT TRÒ CHUYỆN:\n","{summary}\n","\n","TIN NHẮN TRƯỚC CỦA HỆ THỐNG TỚI SINH VIÊN:\n","\"{prev_sys_message}\"\n","\n","TIN NHẮN CỦA SINH VIÊN:\n","\"{user_message}\"\n","\n","CHỈ TRẢ VỀ JSON THEO ĐỊNH DẠNG SAU, KHÔNG GIẢI THÍCH THÊM:\n","{{\n","  \"form_name\": Tên biểu mẫu hoặc null,\n","  \"is_recommend\": true/false hoặc null\n","}}\n","\"\"\"\n","\n","EXTRACT_PROMPT = \"\"\"\n","Bạn là hệ thống Chat Bot hỗ trợ sinh viên Đại học Bách Khoa Hà Nội (HUST), chuyên gia trích xuất dữ liệu.\n","Nhiệm vụ: Tìm kiếm thông tin từ TIN NHẮN MỚI NHẤT và TÓM TẮT TRÒ CHUYỆN để điền vào form.\n","\n","ĐƠN: {form_name}\n","CÁC TRƯỜNG THÔNG TIN CẦN TÌM:\n","{fields_desc}\n","\n","DỮ LIỆU CÓ SẴN:\n","{user_data}\n","\n","TÓM TẮT TRÒ CHUYỆN:\n","{summary}\n","\n","TIN NHẮN TRƯỚC CỦA HỆ THỐNG TỚI SINH VIÊN:\n","\"{prev_sys_message}\"\n","\n","TIN NHẮN CỦA SINH VIÊN:\n","\"{user_message}\"\n","\n","YÊU CẦU:\n","- Ưu tiên thông tin trong tin nhắn mới nhất và lịch sử trò chuyện.\n","- Nếu không có trong chat, mới lấy từ Hồ sơ sinh viên.\n","- Nếu không tìm thấy hoặc không thể suy luận ra, bỏ qua (không bịa đặt).\n","\n","CHỈ TRẢ VỀ JSON THEO ĐỊNH DẠNG SAU, KHÔNG GIẢI THÍCH THÊM:\n","{{\n","  \"values\": {{\"field_key\": \"extracted_value\"}}\n","}}\n","\"\"\"\n","\n","GUIDANCE_PROMPT = \"\"\"\n","THÔNG TIN TRA CỨU TỪ TÀI LIỆU NHÀ TRƯỜNG (RAG):\n","{rag_context}\n","\n","==== KẾT THÚC THÔNG TIN TRA CỨU TỪ TÀI LIỆU NHÀ TRƯỜNG ====\n","\n","Bạn là hệ thống Chat Bot hỗ trợ sinh viên Đại học Bách Khoa Hà Nội (HUST), đóng vai trò Trợ lý hướng dẫn điền biểu mẫu.\n","Sinh viên đang muốn tạo biểu mẫu: {form_description}\n","\n","TÓM TẮT TRÒ CHUYỆN:\n","{summary}\n","\n","TIN NHẮN TRƯỚC CỦA HỆ THỐNG TỚI SINH VIÊN:\n","\"{prev_sys_message}\"\n","\n","TIN NHẮN CỦA SINH VIÊN:\n","\"{user_message}\"\n","\n","TRẠNG THÁI DỮ LIỆU HIỆN TẠI:\n","{current_data_json}\n","\n","DANH SÁCH LỖI CẦN KHẮC PHỤC:\n","{error_list}\n","\n","THÔNG TIN THAM CHIẾU (SPEC):\n","{field_specs}\n","\n","NHIỆM VỤ:\n","- Trả lời LỊCH SỰ, TRỌNG TÂM như cán bộ hỗ trợ sinh viên.\n","- Trả lời bằng ngôn ngữ viết trang trọng và KHÔNG bao gồm các ký hiệu, tên biến, ngôn ngữ lập trình.\n","- Trình diễn câu trả lời dễ đọc, cách dòng, lề đầy đủ.\n","- Giải thích nhanh về biểu mẫu đang tạo.\n","- Hãy thông báo cho người dùng biết dữ liệu nào còn thiếu hoặc sai định dạng.\n","- Giải thích rõ ràng dựa trên \"meaning\" và \"pattern\" (ví dụ: Ngày sinh phải là số, MSSV phải đủ 10-11 số...).\n","- Đồng thời cung cấp các thông tin điền biểu mẫu đã trích xuất được để sinh viên kiểm tra lại.\n","\"\"\"\n","\n","REVIEW_PROMPT = \"\"\"\n","Bạn là hệ thống Chat Bot hỗ trợ sinh viên Đại học Bách Khoa Hà Nội (HUST), đóng vai trò Trợ lý xác nhận.\n","Dữ liệu cho đơn {form_description} đã ĐẦY ĐỦ và HỢP LỆ.\n","\n","FORM: {form_name}\n","CÁC TRƯỜNG THÔNG TIN:\n","{fields_desc}\n","\n","DỮ LIỆU CÓ SẴN:\n","{user_data}\n","\n","TÓM TẮT TRÒ CHUYỆN:\n","{summary}\n","\n","TIN NHẮN TRƯỚC CỦA HỆ THỐNG TỚI SINH VIÊN:\n","\"{prev_sys_message}\"\n","\n","TIN NHẮN CỦA SINH VIÊN:\n","\"{user_message}\"\n","\n","NHIỆM VỤ:\n","- Trả lời LỊCH SỰ, TRỌNG TÂM như cán bộ hỗ trợ sinh viên.\n","- Trả lời bằng ngôn ngữ viết trang trọng và KHÔNG bao gồm các ký hiệu, tên biến, ngôn ngữ lập trình.\n","- Trình diễn câu trả lời dễ đọc, cách dòng, lề đầy đủ.\n","- Liệt kê lại các thông tin trên một cách rõ ràng, đẹp mắt để người dùng kiểm tra.\n","- Hỏi người dùng: \"Thông tin trên đã chính xác chưa? Bạn có muốn tạo đơn ngay không?\"\n","\"\"\"\n","\n","CONFIRM_CHECK_PROMPT = \"\"\"\n","Bạn là hệ thống Chat Bot hỗ trợ sinh viên Đại học Bách Khoa Hà Nội (HUST).\n","Phân tích tin nhắn người dùng để xác định xem họ có ĐỒNG Ý/XÁC NHẬN hành động tạo đơn hay không.\n","CONTEXT: Bot vừa hỏi xác nhận thông tin.\n","\n","ĐƠN: {form_name}\n","\n","TÓM TẮT TRÒ CHUYỆN:\n","{summary}\n","\n","TIN NHẮN TRƯỚC CỦA HỆ THỐNG TỚI SINH VIÊN:\n","\"{prev_sys_message}\"\n","\n","TIN NHẮN CỦA SINH VIÊN:\n","\"{user_message}\"\n","\n","CHỈ TRẢ VỀ JSON THEO ĐỊNH DẠNG SAU, KHÔNG GIẢI THÍCH THÊM:\n","{{\n","  \"confirmed\": true/false\n","}}\n","(Ví dụ: \"ok\", \"đúng rồi\", \"tạo đi\", \"yes\" -> true. \"sửa lại tên\", \"chưa\", \"đợi chút\" -> false)\n","\"\"\"\n","\n","SUMMARIZE_PROMPT = \"\"\"\n","Bạn là **hệ thống Chat Bot hỗ trợ sinh viên Đại học Bách Khoa Hà Nội (HUST)**.\n","Bạn đang thực hiện công việc tóm tắt văn bản hội thoại giữa bạn (Chatbot hỗ trợ) và một sinh viên.\n","LƯU Ý:\n","- Tóm tắt đầy đủ thông tin trong nội dung cùng với cuộc hội thoại\n","- Chú ý ghi lại cụ thể các THÔNG TIN CÁ NHÂN hay học tập mà sinh viên cung cấp, có thể hữu ích cho việc làm đơn.\n","- TRẢ VỀ TRỰC TIẾP NỘI DUNG PHẦN TÓM TẮT, KHÔNG MỞ ĐẦU, TIÊU ĐỀ, ĐỀ MỤC HAY GIẢI THÍCH GÌ THÊM TRƯỚC VÀ SAU ĐẤY.\n","\n","HẪY TÓM TẮT:\n","Nội dung: {old_summary}\n","Sinh viên (chú ý vào câu trả lời của sinh viên vì nó có thể mang thông tin sinh viên): {user_message}\n","Bạn: {bot_response}\n","\"\"\""],"metadata":{"id":"OmLnoQ6o9h5s","executionInfo":{"status":"aborted","timestamp":1769053569096,"user_tz":-420,"elapsed":27309,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FieldSpec(BaseModel):\n","    label: str\n","    required: bool\n","    meaning: str\n","    pattern: Optional[str] = None\n","\n","class FormSpec(BaseModel):\n","    name: str\n","    description: str\n","    fields: Dict[str, FieldSpec]\n","\n","class IntentOutput(BaseModel):\n","    form_name: Optional[str] = None\n","    is_recommend: bool = True\n","    is_direct_command: bool = False\n","\n","class AgentOutput(BaseModel):\n","    answer: str\n","    updated_summary: str\n","    action_taken: str = \"chat\""],"metadata":{"id":"6JUUJy3KGI-k","executionInfo":{"status":"aborted","timestamp":1769053569099,"user_tz":-420,"elapsed":27307,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_forms() -> Dict[str, FormSpec]:\n","    forms = {}\n","    for spec_path in Path(FORMS_PATH).glob(\"*-spec.json\"):\n","        try:\n","            raw = json.loads(spec_path.read_text(encoding=\"utf-8\"))\n","            name = spec_path.stem.replace(\"-spec\", \"\")\n","            fields_raw = raw.get(\"data\", {})\n","\n","            forms[name] = FormSpec(\n","                name=name,\n","                description=raw.get(\"description\", \"\"),\n","                fields={\n","                    k: FieldSpec(**v)\n","                    for k, v in fields_raw.items()\n","                }\n","            )\n","        except Exception as e:\n","            print(f\"ERROR: load form {spec_path}: {e}\")\n","    return forms\n","\n","def prettify_value(value):\n","    if isinstance(value, bool):\n","        return \"Có\" if value else \"Không\"\n","    if value is None:\n","        return \"—\"\n","    return value\n","\n","def dict_format(data: dict) -> str:\n","    lines = []\n","    for section, content in data.items():\n","        lines.append(section.upper())\n","        if isinstance(content, dict):\n","            for k, v in content.items():\n","                lines.append(f\"- {k}: {prettify_value(v)}\")\n","        else:\n","            lines.append(f\"- {prettify_value(content)}\")\n","        lines.append(\"\")\n","    return \"\\n\".join(lines).strip()\n","\n","class HustRAG:\n","    def __init__(self, embedder, reranker, reranker_tokenizer, retrieve_k=30, rerank_k=10):\n","        self.embedder = embedder\n","        self.reranker = reranker\n","        self.reranker_tokenizer = reranker_tokenizer\n","        self.retrieve_k = retrieve_k\n","        self.rerank_k = rerank_k\n","\n","    def search(self, query: str, retrieve_k=None, rerank_k=None) -> str:\n","        retrieve_k = retrieve_k or self.retrieve_k\n","        rerank_k = rerank_k or self.rerank_k\n","        results = pipeline(query, self.embedder, self.reranker, self.reranker_tokenizer, retrieve_k, rerank_k)\n","        context_parts = []\n","        for i, r in enumerate(reversed(results), 1):\n","            context_parts.append(f\"[{i}. Nguồn: {r['doc']['metadata']['source']}]:\\n{r['doc']['text']}\")\n","        return \"\\n\\n\".join(context_parts)"],"metadata":{"id":"AOJQRXnPquQk","executionInfo":{"status":"aborted","timestamp":1769053569102,"user_tz":-420,"elapsed":27305,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class HustChat:\n","    def __init__(self, forms, rag, api_key: str):\n","        self.client = Groq(api_key=api_key)\n","        self.forms = forms\n","        self.rag = rag\n","\n","    def _call_json(self, model, prompt) -> dict:\n","        try:\n","            res = self.client.chat.completions.create(\n","                model=model, messages=[{\"role\": \"user\", \"content\": prompt}],\n","                response_format={\"type\": \"json_object\"}, temperature=0\n","            )\n","            return json.loads(res.choices[0].message.content)\n","        except Exception as e:\n","            print(f\"Error JSON {model}: {e}\")\n","            return {}\n","\n","    def _call_text(self, model, prompt) -> str:\n","        try:\n","            res = self.client.chat.completions.create(\n","                model=model, messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=0.3\n","            )\n","            return res.choices[0].message.content.strip()\n","        except:\n","            return \"\"\n","\n","    def detect_intent(self, user_message: str, prev_ans: str, summary: str = \"\") -> dict:\n","        form_list = \"\\n\".join([f\"- {k}: {v.description}\" for k, v in self.forms.items()])\n","        prompt = INTENT_PROMPT.format(form_list=form_list, prev_sys_message=prev_ans, user_message=user_message, summary=summary)\n","        return self._call_json(INTENT_LLM, prompt)\n","\n","    def extract_data(self, form_name: str, summary: str, user_message: str, prev_ans: str) -> dict:\n","        spec = self.forms[form_name]\n","        fields_desc = \"\\n\".join([f\"- {k}: {v.meaning}\" for k, v in spec.fields.items()])\n","\n","        prompt = EXTRACT_PROMPT.format(\n","            form_name=form_name,\n","            fields_desc=fields_desc,\n","            user_data=dict_format(MOCK_USER_DATA),\n","            summary=summary,\n","            prev_sys_message=prev_ans,\n","            user_message=user_message\n","        )\n","        return self._call_json(EXTRACTOR_LLM, prompt).get(\"values\", {})\n","\n","    def validate_data(self, form_name: str, extracted_data: dict) -> List[str]:\n","        spec = self.forms[form_name]\n","        errors = []\n","\n","        for field_key, field_spec in spec.fields.items():\n","            value = extracted_data.get(field_key)\n","\n","            if field_spec.required and not value:\n","                errors.append(f\"Thiếu thông tin: {field_spec.label} ({field_spec.meaning})\")\n","                continue\n","\n","            if value and field_spec.pattern:\n","                if not re.match(field_spec.pattern, str(value)):\n","                    errors.append(f\"Sai định dạng '{field_spec.label}': Giá trị '{value}' không khớp quy tắc.\")\n","\n","        return errors\n","\n","    def check_user_confirmation(self, form_name: str, summary: str, prev_ans: str, user_message: str) -> bool:\n","        res = self._call_json(CHECKER_LLM, CONFIRM_CHECK_PROMPT.format(form_name=form_name, summary=summary, prev_sys_message=prev_ans, user_message=user_message))\n","        return res.get(\"confirmed\", False)\n","\n","    def generate_form(self, form_name: str, extracted_data: dict) -> str:\n","        template_path = f\"{FORMS_PATH}/{form_name}.docx\"\n","        if not Path(template_path).exists():\n","            return f\"ERROR: Không tìm thấy file mẫu {template_path}\"\n","\n","        doc = DocxTemplate(template_path)\n","        doc.render(extracted_data)\n","\n","        output_path = f\"output/{form_name}.docx\"\n","        doc.save(output_path)\n","        return output_path\n","\n","    def process_message(self, user_message: str, summary: str, current_form_context: str = None, prev_ans: str = \"\") -> dict:\n","        summary = summary or 'Chưa có'\n","        form_name = current_form_context\n","        intent_data = self.detect_intent(user_message, prev_ans, summary)\n","\n","        if intent_data.get(\"form_name\"):\n","            form_name = intent_data[\"form_name\"]\n","\n","        if not form_name:\n","            rag_context = self.rag.search(user_message)\n","        else:\n","            rag_context = self.rag.search(user_message, 20, 1)\n","\n","        if not form_name:\n","            chat_prompt = CHAT_PROMPT.format(\n","                rag_context=rag_context,\n","                user_data=dict_format(MOCK_USER_DATA),\n","                form_list = \"\\n\".join([f\"- {k}: {v.description}\" for k, v in self.forms.items()]),\n","                summary=summary,\n","                prev_sys_message=prev_ans,\n","                user_message=user_message\n","            )\n","\n","            answer = self._call_text(CHAT_LLM, chat_prompt)\n","            new_summary = self._update_summary(rag_context, summary, user_message, answer)\n","            return {\"answer\": answer, \"summary\": new_summary, \"form_context\": None, \"status\": \"chat\", \"rag_context\": rag_context}\n","\n","        elif intent_data[\"is_recommend\"]:\n","            chat_prompt = CHAT_PROMPT.format(\n","                rag_context=rag_context,\n","                user_data=dict_format(MOCK_USER_DATA),\n","                form_list = \"\\n\".join([f\"- {k}: {v.description}\" for k, v in self.forms.items()]),\n","                summary=summary + f\"\\nHệ thống đang gợi ý rằng có thể người dùng muốn tạo đơn {form_name}\",\n","                prev_sys_message=prev_ans,\n","                user_message=user_message\n","            )\n","\n","            answer = self._call_text(CHAT_LLM, chat_prompt)\n","            new_summary = self._update_summary(rag_context, summary, user_message, answer)\n","            return {\"answer\": answer, \"summary\": new_summary, \"form_context\": None, \"status\": \"chat\", \"rag_context\": rag_context}\n","\n","        spec = self.forms[form_name]\n","\n","        extracted_values = self.extract_data(form_name, summary, user_message, prev_ans=prev_ans)\n","\n","        errors = self.validate_data(form_name, extracted_values)\n","\n","        if errors:\n","            field_specs_text = \"\\n\".join([f\"- {k}: {v.meaning} (Format: {v.pattern or 'Tự do'})\" for k, v in spec.fields.items()])\n","\n","            guidance_prompt = GUIDANCE_PROMPT.format(\n","                rag_context=rag_context,\n","                form_description=spec.description,\n","                current_data_json=json.dumps(extracted_values, ensure_ascii=False),\n","                error_list=\"\\n\".join([f\"- {e}\" for e in errors]),\n","                field_specs=field_specs_text,\n","                summary=summary,\n","                prev_sys_message=prev_ans,\n","                user_message=user_message\n","            )\n","\n","            answer = self._call_text(GUIDE_LLM, guidance_prompt)\n","            new_summary = self._update_summary(rag_context, summary, user_message, answer)\n","            return {\"answer\": answer, \"summary\": new_summary, \"form_context\": form_name, \"status\": \"guiding\", \"rag_context\": rag_context}\n","\n","        else:\n","            is_confirmed = self.check_user_confirmation(form_name, summary, prev_ans, user_message)\n","\n","            if is_confirmed:\n","                answer = f\"Đã tạo thành công biểu mẫu tại {self.generate_form(form_name, extracted_values)} theo yêu cầu của sinh viên.\"\n","                new_summary = self._update_summary(rag_context, summary, user_message, \"\\n\" + answer)\n","                return {\"answer\": answer, \"summary\": new_summary, \"form_context\": None, \"status\": \"done\", \"rag_context\": rag_context}\n","            else:\n","                valid_data_view = \"\\n\".join([f\"- {spec.fields[k].label}: {v}\" for k, v in extracted_values.items()])\n","                review_prompt = REVIEW_PROMPT.format(\n","                    form_description=spec.description,\n","                    form_name=form_name,\n","                    fields_desc=\"\\n\".join([f\"- {k}: {v.meaning}\" for k, v in spec.fields.items()]),\n","                    user_data=json.dumps(MOCK_USER_DATA, ensure_ascii=False),\n","                    valid_data_view=valid_data_view,\n","                    summary=summary,\n","                    prev_sys_message=prev_ans,\n","                    user_message=user_message\n","                )\n","                answer = self._call_text(GUIDE_LLM, review_prompt)\n","                new_summary = self._update_summary(rag_context, summary, user_message, answer)\n","                return {\"answer\": answer, \"summary\": new_summary, \"form_context\": form_name, \"status\": \"reviewing\", \"rag_context\": rag_context}\n","\n","    def _update_summary(self, rag_context, old, user, bot):\n","        return self._call_text(SUM_LLM, SUMMARIZE_PROMPT.format(rag_context=rag_context, old_summary=old, user_message=user, bot_response=bot))"],"metadata":{"id":"YB3bjR1BW2zh","executionInfo":{"status":"aborted","timestamp":1769053569106,"user_tz":-420,"elapsed":27303,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"],"metadata":{"id":"wis_ilZCOQmi","executionInfo":{"status":"aborted","timestamp":1769053569108,"user_tz":-420,"elapsed":27300,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Path(\"output\").mkdir(parents=True, exist_ok=True)\n","\n","rag = HustRAG(embedder, reranker, reranker_tokenizer, retrieve_k=15, rerank_k=2)\n","chatbot = HustChat(forms=load_forms(), rag=rag, api_key=GROQ_API_KEY)\n","\n","current_summary = \"\"\n","current_form_context = None\n","history = []\n","result = {\"answer\": \"Chưa có\"}\n","\n","print(\"=== ChatBot Hỗ trợ Sinh viên HUST ===\")\n","print(\"Gõ 'exit' để thoát.\\n\")\n","\n","while True:\n","    user_input = input(\"Sinh viên: \")\n","    if user_input.lower() == \"exit\":\n","        break\n","\n","    result = chatbot.process_message(\n","        user_message=user_input,\n","        summary=current_summary,\n","        current_form_context=current_form_context,\n","        prev_ans=result[\"answer\"]\n","    )\n","\n","    current_summary = result[\"summary\"]\n","    current_form_context = result[\"form_context\"]\n","\n","    history.append({\n","        \"user\": user_input,\n","        \"bot\": result[\"answer\"],\n","        \"status\": result[\"status\"],\n","        \"summary\": result[\"summary\"],\n","        \"rag_context\": result[\"rag_context\"],\n","        \"form\": current_form_context\n","    })\n","\n","    print(f\"\\nChatBot:\\n{result['answer']}\\n\")\n","    print(\"-\" * 50)"],"metadata":{"id":"lj2x2nxwFOnK","executionInfo":{"status":"aborted","timestamp":1769053569111,"user_tz":-420,"elapsed":27298,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for item in history:\n","    print(\"=\" * 200)\n","    print(\"=\" * 200)\n","    for key, value in item.items():\n","        print(f\"\\n=== {key.upper()} ===\")\n","        print(value)"],"metadata":{"id":"WCOnwJD2GKd7","executionInfo":{"status":"aborted","timestamp":1769053569113,"user_tz":-420,"elapsed":27295,"user":{"displayName":"Nguyễn Bảo Ngọc","userId":"11079399998754743568"}},"collapsed":true},"execution_count":null,"outputs":[]}]}